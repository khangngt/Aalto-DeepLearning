{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This code is taken from https://github.com/arundasan91/MagnaTagATune. Credit to Arun Das."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import required libraries\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import os\n",
    "import shutil\n",
    "import librosa\n",
    "\n",
    "# Import MatPlotLib Package\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Set number of columns to show in the notebook\n",
    "pd.set_option('display.max_columns', 200)\n",
    "# Set number of rows to show in the notebook\n",
    "pd.set_option('display.max_rows', 50) \n",
    "# Make the graphs a bit prettier\n",
    "plt.style.use('ggplot')\n",
    "\n",
    "# Display pictures within the notebook itself\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Read the annotations file\n",
    "newdata = pd.read_csv('annotations_final.csv', sep=\"\\t\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Display the top 5 rows\n",
    "newdata.head(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get to know the data better\n",
    "newdata.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# What colums are there ?\n",
    "newdata.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Extract the clip_id and mp3_path\n",
    "newdata[[\"clip_id\", \"mp3_path\"]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Previous command extracted it as a Dataframe. We need it as a matrix to do analyics on. \n",
    "# Extract clip_id and mp3_path as a matrix.\n",
    "clip_id, mp3_path = newdata[[\"clip_id\", \"mp3_path\"]].as_matrix()[:,0], newdata[[\"clip_id\", \"mp3_path\"]].as_matrix()[:,1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Some of the tags in the dataset are really close to each other. Lets merge them together\n",
    "synonyms = [['beat', 'beats'],\n",
    "            ['chant', 'chanting'],\n",
    "            ['choir', 'choral'],\n",
    "            ['classical', 'clasical', 'classic'],\n",
    "            ['drum', 'drums'],\n",
    "            ['electro', 'electronic', 'electronica', 'electric'],\n",
    "            ['fast', 'fast beat', 'quick'],\n",
    "            ['female', 'female singer', 'female singing', 'female vocals', 'female vocal', 'female voice', 'woman', 'woman singing', 'women'],\n",
    "            ['flute', 'flutes'],\n",
    "            ['guitar', 'guitars'],\n",
    "            ['hard', 'hard rock'],\n",
    "            ['harpsichord', 'harpsicord'],\n",
    "            ['heavy', 'heavy metal', 'metal'],\n",
    "            ['horn', 'horns'],\n",
    "            ['india', 'indian'],\n",
    "            ['jazz', 'jazzy'],\n",
    "            ['male', 'male singer', 'male vocal', 'male vocals', 'male voice', 'man', 'man singing', 'men'],\n",
    "            ['no beat', 'no drums'],\n",
    "            ['no singer', 'no singing', 'no vocal','no vocals', 'no voice', 'no voices', 'instrumental'],\n",
    "            ['opera', 'operatic'],\n",
    "            ['orchestra', 'orchestral'],\n",
    "            ['quiet', 'silence'],\n",
    "            ['singer', 'singing'],\n",
    "            ['space', 'spacey'],\n",
    "            ['string', 'strings'],\n",
    "            ['synth', 'synthesizer'],\n",
    "            ['violin', 'violins'],\n",
    "            ['vocal', 'vocals', 'voice', 'voices'],\n",
    "            ['strange', 'weird']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Merge the synonyms and drop all other columns than the first one.\n",
    "\"\"\"\n",
    "Example:\n",
    "Merge 'beat', 'beats' and save it to 'beat'.\n",
    "Merge 'classical', 'clasical', 'classic' and save it to 'classical'.\n",
    "\"\"\"\n",
    "for synonym_list in synonyms:\n",
    "    newdata[synonym_list[0]] = newdata[synonym_list].max(axis=1)\n",
    "    newdata.drop(synonym_list[1:], axis=1, inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Did it work ?\n",
    "newdata.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Lets view it.\n",
    "newdata.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Drop the mp3_path tag from the dataframe\n",
    "newdata.drop('mp3_path', axis=1, inplace=True)\n",
    "# Save the column names into a variable\n",
    "data = newdata.sum(axis=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Find the distribution of tags.\n",
    "data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Sort the column names.\n",
    "data.sort_values(axis=0, inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Find the top tags from the dataframe.\n",
    "topindex, topvalues = list(data.index[84:]), data.values[84:]\n",
    "del(topindex[-1])\n",
    "topvalues = np.delete(topvalues, -1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get the top column names\n",
    "topindex"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get only the top column values\n",
    "topvalues"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get a list of columns to remove\n",
    "rem_cols = data.index[:84]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cross-check: How many columns are we removing ?\n",
    "len(rem_cols)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Drop the columns that needs to be removed\n",
    "newdata.drop(rem_cols, axis=1, inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "newdata.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a backup of the dataframe\n",
    "backup_newdata = newdata"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Shuffle the dataframe\n",
    "from sklearn.utils import shuffle\n",
    "newdata = shuffle(newdata, random_state = 42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "newdata.reset_index(drop=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# One final check\n",
    "newdata.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Let us save the final columns\n",
    "final_columns_names = list(newdata.columns)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Do it only once to delete the clip_id column\n",
    "del(final_columns_names[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Verified\n",
    "final_columns_names"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create the file which is to be saved off (you could skip and apply similar steps in the previous dataframe)\n",
    "# Here, binary 0's and 1's from each column is changed to 'False' and 'True' by using '==' operator on the dataframe.\n",
    "final_matrix = pd.concat([newdata['clip_id'], newdata[final_columns_names]==1], axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Rename all the mp3 files to their clip_id and save it into one folder named 'dataset_clip_id_mp3' in the same folder.\n",
    "# Get the current working directory\n",
    "root = os.getcwd()\n",
    "os.mkdir(\"D:/dataset_clip_id_mp3/\", 0o755 )\n",
    "\n",
    "# Iterate over the mp3 files, rename them to the clip_id and save it to another folder.\n",
    "for id in range(25863):\n",
    "    #print clip_id[id], mp3_path[id]\n",
    "    src = \"D:/mp3/\" + mp3_path[id]\n",
    "    dest = \"D:/dataset_clip_id_mp3/\" + str(clip_id[id]) + \".mp3\"\n",
    "    shutil.copy2(src,dest)\n",
    "    #print src,dest"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_melgram(audio_path):\n",
    "    ''' Compute a mel-spectrogram and returns it in a shape of (1,1,96,1366), where\n",
    "    96 == #mel-bins and 1366 == #time frame\n",
    "    parameters\n",
    "    ----------\n",
    "    audio_path: path for the audio file.\n",
    "                Any format supported by audioread will work.\n",
    "    More info: http://librosa.github.io/librosa/generated/librosa.core.load.html#librosa.core.load\n",
    "    '''\n",
    "\n",
    "    # mel-spectrogram parameters\n",
    "    SR = 12000\n",
    "    N_FFT = 512\n",
    "    N_MELS = 96\n",
    "    HOP_LEN = 256\n",
    "    DURA = 29.12  # to make it 1366 frame..\n",
    "\n",
    "    src, sr = librosa.load(audio_path, sr=SR)  # whole signal\n",
    "    n_sample = src.shape[0]\n",
    "    n_sample_fit = int(DURA*SR)\n",
    "\n",
    "    if n_sample < n_sample_fit:  # if too short\n",
    "        src = np.hstack((src, np.zeros((int(DURA*SR) - n_sample,))))\n",
    "    elif n_sample > n_sample_fit:  # if too long\n",
    "        src = src[(n_sample-n_sample_fit)//2:(n_sample+n_sample_fit)//2]\n",
    "    logam = librosa.logamplitude\n",
    "    melgram = librosa.feature.melspectrogram\n",
    "    ret = logam(melgram(y=src, sr=SR, hop_length=HOP_LEN,\n",
    "                        n_fft=N_FFT, n_mels=N_MELS)**2,\n",
    "                ref_power=1.0)\n",
    "    ret = ret[np.newaxis, np.newaxis, :]\n",
    "    return ret"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get the absolute path of all audio files and save it to audio_paths array\n",
    "audio_paths = []\n",
    "# Variable to save the mp3 files that don't work\n",
    "files_that_dont_work=[]\n",
    "os.chdir('D:/ex')\n",
    "root = os.getcwd()\n",
    "os.chdir('D:/dataset_clip_id_mp3/')\n",
    "for audio_path in os.listdir('.'):\n",
    "    #audio_paths.append(os.path.abspath(fname))\n",
    "    if os.path.isfile('D:/dataset_clip_id_melgram/' + str(os.path.splitext(audio_path)[0]) + '.npy'):\n",
    "        #print \"existtt\"\n",
    "        continue\n",
    "    else:\n",
    "        if str(os.path.splitext(audio_path)[1]) == \".mp3\":\n",
    "            try:\n",
    "                melgram = compute_melgram(os.path.abspath(audio_path))\n",
    "                dest = 'D:/dataset_clip_id_melgram/' + str(os.path.splitext(audio_path)[0])\n",
    "                np.save(dest, melgram)\n",
    "            except EOFError:\n",
    "                files_that_dont_work.append(audio_path)\n",
    "                continue"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get a list of \n",
    "mp3_available = []\n",
    "melgram_available = []\n",
    "for mp3 in os.listdir('D:/dataset_clip_id_mp3/'):\n",
    "     mp3_available.append(int(os.path.splitext(mp3)[0]))\n",
    "        \n",
    "for melgram in os.listdir('D:/dataset_clip_id_melgram/'):\n",
    "     melgram_available.append(int(os.path.splitext(melgram)[0]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# The latest clip_id\n",
    "new_clip_id = final_matrix['clip_id']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Let us see which all files have not been converted into melspectrograms.\n",
    "set(list(new_clip_id)).difference(melgram_available)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Saw that these clips were extra 35644, 55753, 57881. Removing them.\n",
    "final_matrix = final_matrix[final_matrix['clip_id']!= 35644]\n",
    "final_matrix = final_matrix[final_matrix['clip_id']!= 55753]\n",
    "final_matrix = final_matrix[final_matrix['clip_id']!= 57881]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check again\n",
    "final_matrix.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save the matrix\n",
    "final_matrix.to_pickle('D:/final_Dataframe.pkl')\n",
    "# This is how you can load it afterwards. final_matrix = pd.read_pickle('final_Dataframe.pkl')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Seperate the training, test and validation dataframe.\n",
    "training_with_clip = final_matrix[:19773]\n",
    "validation_with_clip = final_matrix[19773:21294]\n",
    "testing_with_clip = final_matrix[21294:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Quick peek\n",
    "training_with_clip"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Quick peek\n",
    "testing_with_clip"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Quick peek\n",
    "validation_with_clip"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Extract the corresponding clip_id's\n",
    "training_clip_id = training_with_clip['clip_id'].values\n",
    "validation_clip_id = validation_with_clip['clip_id'].values\n",
    "testing_clip_id = testing_with_clip['clip_id'].values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check !\n",
    "training_clip_id"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Go to the directory you want to save the dataframe\n",
    "os.chdir('D:/final_dataset')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save the 'y' values.\n",
    "np.save('train_y.npy', training_with_clip[final_columns_names].values)\n",
    "np.save('valid_y.npy', validation_with_clip[final_columns_names].values)\n",
    "np.save('test_y.npy', testing_with_clip[final_columns_names].values)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save the 'x' clip_id's. We will make the numpy array using this.\n",
    "np.savetxt('train_x_clip_id.txt', training_with_clip['clip_id'].values, fmt='%i')\n",
    "np.savetxt('test_x_clip_id.txt', testing_with_clip['clip_id'].values, fmt='%i')\n",
    "np.savetxt('valid_x_clip_id.txt', validation_with_clip['clip_id'].values, fmt='%i')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Now to combine the melgrams according to the clip_id.\n",
    "\n",
    "# Variable to store melgrams.\n",
    "train_x = np.zeros((0, 1, 96, 1366))\n",
    "test_x = np.zeros((0, 1, 96, 1366))\n",
    "valid_x = np.zeros((0, 1, 96, 1366))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "root = 'D:/'\n",
    "os.chdir(root + \"/dataset_clip_id_melgram/\")\n",
    "for i,valid_clip in enumerate(list(validation_clip_id)):\n",
    "    if os.path.isfile(str(valid_clip) + '.npy'):\n",
    "        #print i,valid_clip\n",
    "        melgram = np.load(str(valid_clip) + '.npy')\n",
    "        valid_x = np.concatenate((valid_x, melgram), axis=0)\n",
    "os.chdir('D:/sets/')\n",
    "np.save('valid_x.npy', valid_x)\n",
    "print(\"Validation file created\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "root = 'D:/'\n",
    "os.chdir(root + \"/dataset_clip_id_melgram/\")\n",
    "for i,test_clip in enumerate(list(testing_clip_id)):\n",
    "    if os.path.isfile(str(test_clip) + '.npy'):\n",
    "        #print i,test_clip\n",
    "        melgram = np.load(str(test_clip) + '.npy')\n",
    "        test_x = np.concatenate((test_x, melgram), axis=0)\n",
    "os.chdir('D:/sets/')\n",
    "np.save('test_x.npy', test_x)\n",
    "print(\"Testing file created\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "root = 'D:/'\n",
    "os.chdir(root + \"/dataset_clip_id_melgram/\")\n",
    "for i,train_clip in enumerate(list(training_clip_id)):\n",
    "    ##if os.path.isfile(str(train_clip) + '.npy'):\n",
    "        ##print i,train_clip\n",
    "    #melgram = compute_melgram(str(train_clip) + '.mp3')\n",
    "    ##melgram = np.load(str(train_clip) + '.npy')\n",
    "    if os.path.isfile(str(train_clip) + '.npy'):\n",
    "        melgram = np.load(str(train_clip) + '.npy')\n",
    "        train_x = np.concatenate((train_x, melgram), axis=0)\n",
    "os.chdir('D:/sets/')\n",
    "np.save('train_x.npy', train_x)\n",
    "print(\"Training file created.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
